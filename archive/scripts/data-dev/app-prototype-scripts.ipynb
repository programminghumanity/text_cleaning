{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "#text = textract.process(\"sample_pdf_french_law.pdf\")\n",
    "\n",
    "pdfFileObj = open('sample_pdf_french_law.pdf', 'rb')\n",
    "# read object\n",
    "\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "\n",
    "# print number of pages\n",
    "print(pdfReader.numPages)\n",
    "\n",
    "pageObj = pdfReader.getPage(1)\n",
    "print(pageObj.extractText()[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import digits \n",
    "    \n",
    "remove_digits = str.maketrans('', '', digits) \n",
    "res = some_text.translate(remove_digits) \n",
    "  \n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "def remove_URL(sample):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    string = re.sub(r\"http\\S+\", \"\", sample)\n",
    "    string = re.sub(r\"www.\\S+\", \"\", string)\n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try1 = \"www.facebook.com\"\n",
    "try2 = \"http://facebook.com\"\n",
    "try3 = \"https://facebook.com\"\n",
    "try4 = \"twitter.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_URL(try1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strip white spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# You will have to download the set of stop words the first time\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = stopwords.words('english')\n",
    "text = '''\n",
    "In computing, stop words are words which are filtered out before or after \n",
    "processing of natural language data (text). Though \"stop words\" usually \n",
    "refers to the most common words in a language, there is no single universal \n",
    "list of stop words used by all natural language processing tools, and \n",
    "indeed not all tools even use such a list. Some tools specifically avoid \n",
    "removing these stop words to support phrase search.\n",
    "'''\n",
    "print(\"\\nOriginal string:\")\n",
    "print(text)\n",
    "clean_word_list = [word for word in text.split() if word not in stoplist]\n",
    "print(\"\\nAfter removing stop words from the said text:\")\n",
    "print(clean_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "text = \"Nick likes to play football, however he is not too fond of tennis.\"\n",
    "filtered_sentence = remove_stopwords(text)\n",
    "\n",
    "print(filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "strip_multiple_whitespaces(\"sal    ut\" + '\\r' + \" les\" + '\\n' + \"         loulous!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.parsing.preprocessing import strip_tags\n",
    "\n",
    "def remove_tags(inStr):\n",
    "    \"\"\"Removes all the tags and markup e.g. <p> </p>.\"\"\"\n",
    "    return strip_tags(inStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_tags(\"<h1>HAHAHA</h1>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US / UK English normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "def normalize_english(inStr, target):\n",
    "    \"\"\"Normalizes enlish to the target language (UK or US) based\n",
    "    on the dictionary found here: http://www.tysto.com/uk-us-spelling-list.html\"\"\"\n",
    "\n",
    "    # import the matching table\n",
    "    df = pd.read_csv(\"https://raw.githubusercontent.com/sagepublishing/text_cleaning/app_prototype/app_scripts/clean/normalize_english/us_uk_matching_table.csv\")\n",
    "\n",
    "    # create dictionaries (faster to iterate on)\n",
    "    translate_uk_us = dict(zip(df.UK, df.US))\n",
    "    translate_us_uk = dict(zip(df.US, df.UK))\n",
    "\n",
    "    # get target language inputted by user\n",
    "    #target = opts['target_language']\n",
    "\n",
    "    # depending on target language, search and match relevant dict\n",
    "    if target == \"UK\":\n",
    "        for item in translate_us_uk.keys():\n",
    "            if inStr[0].isupper():\n",
    "                return re.sub(item, translate_us_uk[item], inStr.lower()).capitalize()\n",
    "            else:\n",
    "                return re.sub(item, translate_us_uk[item], inStr)\n",
    "\n",
    "    elif target ==\"US\":\n",
    "        for item in translate_uk_us.keys():\n",
    "            if inStr[0].isupper():\n",
    "                return re.sub(item, translate_uk_us[item], inStr.lower()).capitalize()\n",
    "            else:\n",
    "                return re.sub(item, translate_uk_us[item], inStr)\n",
    "            \n",
    "    else:\n",
    "        return \"Wrong input - please enter \\\"US\\\" or \\\"UK\\\".\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"accessorize\"\n",
    "\n",
    "normalize_english(string, \"df\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftfy.fix_encoding(\"Ãºnico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://github.com/jfilter/clean-text/\n",
    "\n",
    "import re\n",
    "\n",
    "URL_REGEX = re.compile(\n",
    "    r\"(?:^|(?<![\\w\\/\\.]))\"\n",
    "    # protocol identifier\n",
    "    # r\"(?:(?:https?|ftp)://)\"  <-- alt?\n",
    "    r\"(?:(?:https?:\\/\\/|ftp:\\/\\/|www\\d{0,3}\\.))\"\n",
    "    # user:pass authentication\n",
    "    r\"(?:\\S+(?::\\S*)?@)?\" r\"(?:\"\n",
    "    # IP address exclusion\n",
    "    # private & local networks\n",
    "    r\"(?!(?:10|127)(?:\\.\\d{1,3}){3})\"\n",
    "    r\"(?!(?:169\\.254|192\\.168)(?:\\.\\d{1,3}){2})\"\n",
    "    r\"(?!172\\.(?:1[6-9]|2\\d|3[0-1])(?:\\.\\d{1,3}){2})\"\n",
    "    # IP address dotted notation octets\n",
    "    # excludes loopback network 0.0.0.0\n",
    "    # excludes reserved space >= 224.0.0.0\n",
    "    # excludes network & broadcast addresses\n",
    "    # (first & last IP address of each class)\n",
    "    r\"(?:[1-9]\\d?|1\\d\\d|2[01]\\d|22[0-3])\"\n",
    "    r\"(?:\\.(?:1?\\d{1,2}|2[0-4]\\d|25[0-5])){2}\"\n",
    "    r\"(?:\\.(?:[1-9]\\d?|1\\d\\d|2[0-4]\\d|25[0-4]))\"\n",
    "    r\"|\"\n",
    "    # host name\n",
    "    r\"(?:(?:[a-z\\\\u00a1-\\\\uffff0-9]-?)*[a-z\\\\u00a1-\\\\uffff0-9]+)\"\n",
    "    # domain name\n",
    "    r\"(?:\\.(?:[a-z\\\\u00a1-\\\\uffff0-9]-?)*[a-z\\\\u00a1-\\\\uffff0-9]+)*\"\n",
    "    # TLD identifier\n",
    "    r\"(?:\\.(?:[a-z\\\\u00a1-\\\\uffff]{2,}))\" r\")\"\n",
    "    # port number\n",
    "    r\"(?::\\d{2,5})?\"\n",
    "    # resource path\n",
    "    r\"(?:\\/[^\\)\\]\\}\\s]*)?\",\n",
    "    # r\"(?:$|(?![\\w?!+&\\/\\)]))\",\n",
    "    # @jfilter: I removed the line above from the regex because I don't understand what it is used for, maybe it was useful?\n",
    "    # But I made sure that it does not include ), ] and } in the URL.\n",
    "    flags=re.UNICODE | re.IGNORECASE,\n",
    ")\n",
    "\n",
    "def replace_urls(inStr, token):\n",
    "    \"\"\"Replace all URLs in ``text`` str with ``replace_with`` str.\"\"\"\n",
    "    return URL_REGEX.sub(token, inStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_urls(\"blablabla https://facebook.com sdfsdfs\", \"<URL>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replace_digits(inStr, token):\n",
    "    \"\"\"Replace all digits in ``text`` str with ``replace_with`` str, i.e., 123.34 to 000.00\"\"\"\n",
    "    return re.sub(r\"\\d\", token, inStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_digits(\"dsfdfs 13878 dflsdfs8\", \"NUM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://github.com/jfilter/clean-text/\n",
    "\n",
    "import re\n",
    "\n",
    "CURRENCIES = {\n",
    "    \"$\": \"USD\",\n",
    "    \"zł\": \"PLN\",\n",
    "    \"£\": \"GBP\",\n",
    "    \"¥\": \"JPY\",\n",
    "    \"฿\": \"THB\",\n",
    "    \"₡\": \"CRC\",\n",
    "    \"₦\": \"NGN\",\n",
    "    \"₩\": \"KRW\",\n",
    "    \"₪\": \"ILS\",\n",
    "    \"₫\": \"VND\",\n",
    "    \"€\": \"EUR\",\n",
    "    \"₱\": \"PHP\",\n",
    "    \"₲\": \"PYG\",\n",
    "    \"₴\": \"UAH\",\n",
    "    \"₹\": \"INR\",\n",
    "}\n",
    "\n",
    "CURRENCY_REGEX = re.compile(\n",
    "    \"({})+\".format(\"|\".join(re.escape(c) for c in CURRENCIES.keys()))\n",
    ")\n",
    "\n",
    "\n",
    "def replace_currency_symbols(inStr, token):\n",
    "    \"\"\"\n",
    "    Replace all currency symbols in ``text`` str with string specified by ``replace_with`` str.\n",
    "    Args:\n",
    "        text (str): raw text\n",
    "        replace_with (str): if None (default), replace symbols with\n",
    "            their standard 3-letter abbreviations (e.g. '$' with 'USD', '£' with 'GBP');\n",
    "            otherwise, pass in a string with which to replace all symbols\n",
    "            (e.g. \"*CURRENCY*\")\n",
    "    Returns:\n",
    "        str\n",
    "    \"\"\"\n",
    "    if token is None:\n",
    "        for k, v in constants.CURRENCIES.items():\n",
    "            text = text.replace(k, v)\n",
    "        return text\n",
    "    else:\n",
    "        return CURRENCY_REGEX.sub(token, inStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_currency_symbols(\"sdfsdfs\", \"<CUR>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I be love it . But certainly I want to check if it work .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/alix/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/alix/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "print(lemmatize_sentence(\"I am loving it. But certainly I wanted to check if it works.\")) #I be love it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sage",
   "language": "python",
   "name": "sage"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
