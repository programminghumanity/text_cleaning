# Schedule and Tasks



## Week 1



### Goals

- [x] Organise GitHub
- [x] Call with TextClean team
- [x] Research corpora
- [x] Research needs of polsci academics
- [x] Kick off script research
- [x] Coordinate with Regina 



### Schedule



| Day       | Hours | Tasks completed                                              |
| --------- | ----- | ------------------------------------------------------------ |
| Monday    | 0     | Set up + admin                                               |
| Tuesday   | 2     | Set up some folders in GitHub repo<br />Started research on functionalities and corpora |
| Wednesday | 2     | Call for 45 minutes<br />Looked through some corpora including EU-related |
| Thursday  | 8     | Looked into parsing PDFs and issue with regular expressions<br />Found some scripts + examples<br />Looked into parsing HTML/XML<br />Found some useful resources for text as data<br />Found some new corpora and created a big list of good corpora<br />Quick look into licensing, see GitHub issue<br />Skimming academic papers to check where they got their data<br />Looked into corpus about IOs but decided to leave for later<br />Painful few hours navigating French archives and conditions of use |
| Friday    | 8     | Looking into Ireland, Wales, Scotland, and Northern Ireland<br />Looking into local level (starting with London)<br />Went through the huge PolData list<br />Went through Data is Plural list<br />Went through GitHub repos<br />Added media and identity & culture corpora |





## Week 2



### Goals

- [ ] Select most different corpora for test
- [ ] Test each file type
- [ ] Look at what cleaning steps are needed
- [ ] Fill gaps of neat packages with small specific scripts



### Schedule



| Day       | Hours | Tasks completed                                              |
| --------- | ----- | ------------------------------------------------------------ |
| Monday    | 0     | Call with Regina                                             |
| Tuesday   | 2     | Call with Daniela to set next steps<br />Looked through corpora to select test corpora<br />Some fixes and addition to the list |
| Wednesday | 4     | Tried UK Parliament data, used get request, formatted into json: text is already nice and clean with relevant covariates attached in pd df<br />Looked into a PDF example, much more challenging<br />Looked through all well packaged modules, took note of shortcomings<br />Explored scripts on GitHub (e.g. the French Law one) |
| Thursday  | 4     | Going through GitHub, blogs and StackOverflow to find ways to convert PDF into more readable format<br />Tried some code snippets<br />Read through some methodology lit to get a better idea of the process<br />Read through documentation of most promising parsing libraries listed in .md file and of tika |
| Friday    | 6     | Worked on pdf text mining: text extraction (easy) + extraction of headers using various libraries + identification of text blocks/paragraphs <br /><br />Looked for python scripts that implement the pdfminer module (instead of using the command line tool which is inconvenient)<br />Debugged some scripts (need to look into pdfminer versions + maybe how to install / uninstall packages easily depending on what the script uses)<br />Tried better extraction of pdf structure by converting to html, would only work for relatively simple PDFs <br />XML text parsing using Element Tree + conversion to dictionary (JSON) for ease of use<br />Ocean monthly meeting for an hour<br />Went through the resources shared by Regina, tried some of the code<br />Found and recorded some code snippets |

