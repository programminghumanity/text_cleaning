{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text cleaning\n",
    "\n",
    "Aim is to try different scripts and libraries to clean text of various formats. \n",
    "\n",
    "**Don't forget to install the modules in requirements.txt**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF\n",
    "\n",
    "Code_40.pdf is the French environmental code. Good example of how bills, articles, amendments, or treaties look like as a PDF (versus letter or manifesto, a lot of different heading, long documents etc.).\n",
    "\n",
    "* PDF format\n",
    "* Just text but very structured, no tables or weird formatting\n",
    "* Will try to extract the text and categories based on the titles and headings\n",
    "* Would be interesting to merge the pdfs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With PyPDF2\n",
    "\n",
    "This works fine but only extracts text, no structure. Ideally want to get headings etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "#text = textract.process(\"code/Code_40.pdf\")\n",
    "\n",
    "pdfFileObj = open('code/Code_40.pdf', 'rb')\n",
    "# read object\n",
    "\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "\n",
    "# print number of pages\n",
    "print(pdfReader.numPages)\n",
    "\n",
    "pageObj = pdfReader.getPage(1)\n",
    "print(pageObj.extractText())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structure of the document:**\n",
    "\n",
    "Just from the raw text we can see the structure: \n",
    "* Each BOOK starts like this: BOOK I\\nCommon provisions\\n\\nArticles L121-1 to\\nL110-2\\n\\n\n",
    "* ENVIRONMENTAL CODE on top left of each page\n",
    "* \\n (line breaks) indicate paragraphs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With PDFminer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Useful scripts:**\n",
    "* convert pdf: https://gist.github.com/terencezl/61fe3f28c44a763dd1e9f060b8ff6f2e\n",
    "* get tags: https://gist.github.com/joelhsmith/5e6ec7ee70ab4b89d7bc5700e9e07fde\n",
    "* converting to html: https://stackoverflow.com/questions/3637781/converting-a-pdf-to-text-html-in-python-so-i-can-parse-it\n",
    "\n",
    "**Problem**: pdfminer is not maintained, all those scripts use deprecated functions/modules. Tried pdfminer.six and pdfminer.3k but same story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pdfminer.converter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-4ad019ec3cb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# a lot of version problems between scripts and package installed via pip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpdfminer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdfinterp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPDFResourceManager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPDFPageInterpreter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpdfminer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextConverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXMLConverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHTMLConverter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpdfminer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLAParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpdfminer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdfpage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPDFPage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pdfminer.converter'"
     ]
    }
   ],
   "source": [
    "# INTERNAL ERRORS using pdfminer - doesn't look well maintained\n",
    "# a lot of version problems between scripts and package installed via pip\n",
    "\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter, XMLConverter, HTMLConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import BytesIO\n",
    "import ply \n",
    "\n",
    "def convert_pdf(path, format='text', password=''):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = BytesIO()\n",
    "    laparams = LAParams()\n",
    "    if format == 'text':\n",
    "        device = TextConverter(rsrcmgr, retstr, laparams=laparams)\n",
    "    elif format == 'html':\n",
    "        device = HTMLConverter(rsrcmgr, retstr, laparams=laparams)\n",
    "    elif format == 'xml':\n",
    "        device = XMLConverter(rsrcmgr, retstr, laparams=laparams)\n",
    "    else:\n",
    "        raise ValueError('provide format, either text, html or xml!')\n",
    "    \n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "    \n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password, caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    text = retstr.getvalue().decode()\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to text, print first 100 chars\n",
    "convert_pdf('Code_40.pdf')[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Tika\n",
    "\n",
    "This works really well in terms of keeping the structure (still need to figure out how to extract the different parts e.g. separate BOOK I and BOOK II). \n",
    "\n",
    "Very slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ENVIRONMENTAL CODE\n",
      "\n",
      "With the cooperation of Michael Faure\n",
      "Professor of Comparative and International Environmental Law and Academic Director of METRO, the Institute for\n",
      "Transnational Legal Research of the Universiteit Maastricht.\n",
      "\n",
      "BOOK I\n",
      "Common provisions Articles L121-1 to\n",
      "\n",
      "L110-2\n",
      "Article L110-1\n",
      "(Act no. 2002-276 of 27 February 2002 Article 132 Official Journal of 28 February 2002)\n",
      "       I. - Natural areas, resources and habitats, sites and la\n"
     ]
    }
   ],
   "source": [
    "from tika import parser\n",
    "\n",
    "raw = parser.from_file('Code_40.pdf')\n",
    "raw.keys()\n",
    "print(raw['content'][50:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  In addition, the National Public Debate Commission ensures the upkeep of good conditions for informing the public\n",
      "</p>\n",
      "<p>Updated 04/10/2006 - Page 1/201</p>\n",
      "<p />\n",
      "</div>\n",
      "<div class=\"page\"><p />\n",
      "<p>ENVIRONMENTAL CODE\n",
      "throughout the implementation phase of the projects referred to it, up to the receipt of equipment and works.\n",
      "       This Commission advises the competent authorities and any developer, at their request, on any question relating to\n",
      "dialogue with the public throughout the development of the project.\n",
      "       The National Public Debate Commission is also entrusted with the role of issuing all and any opinions and\n",
      "recommendations of a general or methodological nature likely to encourage and develop dialogue with the public.\n",
      "       The National Public Debate Commission and individual commissions do not comment on the substance of the\n",
      "projects submitted to them.\n",
      "</p>\n",
      "<p>Article L121-2\n",
      "(Act no. 2002-276 of 27 February 2002 Article 134 Official Journal of 28 February 2002)\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "# setting xmlContent=True adds the html markup which can be useful to detect titles, paragraphs etc.\n",
    "# can then separate the parts using custom script e.g. https://cbrownley.wordpress.com/2016/06/26/parsing-pdfs-in-python-with-tika/\n",
    "\n",
    "raw_xml = parser.from_file('Code_40.pdf', xmlContent=True)\n",
    "print(raw_xml['content'][6000:7000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyMuPDF\n",
    "\n",
    "Good explanation here: https://towardsdatascience.com/extracting-headers-and-paragraphs-from-pdf-using-pymupdf-676e8421c467\n",
    "\n",
    "Most advanced library to extract headings / structure so far, but not great with messy PDFs - not as straightforward as it seems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = fitz.open(\"Code_40.pdf\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31.190000534057617,\n",
       " 346.58001708984375,\n",
       " 534.1900634765625,\n",
       " 387.25,\n",
       " '                CHAPTER II\\n                Environmental evaluation Articles L122-1 to\\nL122-11',\n",
       " 4,\n",
       " 0)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = doc[3]\n",
    "text = page.getText(\"blocks\") # can also use html, dict, xml, xhtml, raw text, blocks works pretty well with list of articles / bills\n",
    "text[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON\n",
    "\n",
    "GET request to UK Parliament API to see what it returns (how clean, how straightforward it is etc.).\n",
    "\n",
    "Very easy to use, text is clean, metadata is easy to store in panda df. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"http://lda.data.parliament.uk/lordswrittenquestions.json?_view=Written+Questions&_pageSize=500&_page=0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_text(response):\n",
    "    \n",
    "    response_json = json.loads(response.text)['result']['items']\n",
    "    df = pd.DataFrame({'AnswerDate': [response_json[i]['AnswerDate']['_value'] for i in range(len(response_json))],\n",
    "                       'AnsweringBody': [response_json[i]['AnsweringBody'][0]['_value'] for i in range(len(response_json))],\n",
    "                       'questionText': [response_json[i]['questionText'] for i in range(len(response_json))],\n",
    "                       'tablingMember': [response_json[i]['tablingMemberPrinted'][0]['_value'] for i in range(len(response_json))]})\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_text(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sage",
   "language": "python",
   "name": "sage"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
