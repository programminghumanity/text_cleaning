{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "#text = textract.process(\"sample_pdf_french_law.pdf\")\n",
    "\n",
    "pdfFileObj = open('sample_pdf_french_law.pdf', 'rb')\n",
    "# read object\n",
    "\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "\n",
    "# print number of pages\n",
    "print(pdfReader.numPages)\n",
    "\n",
    "pageObj = pdfReader.getPage(1)\n",
    "print(pageObj.extractText()[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import digits \n",
    "    \n",
    "remove_digits = str.maketrans('', '', digits) \n",
    "res = some_text.translate(remove_digits) \n",
    "  \n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "def remove_URL(sample):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    string = re.sub(r\"http\\S+\", \"\", sample)\n",
    "    string = re.sub(r\"www.\\S+\", \"\", string)\n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try1 = \"www.facebook.com\"\n",
    "try2 = \"http://facebook.com\"\n",
    "try3 = \"https://facebook.com\"\n",
    "try4 = \"twitter.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_URL(try1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strip white spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/alix/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# You will have to download the set of stop words the first time\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original string:\n",
      "\n",
      "In computing, stop words are words which are filtered out before or after \n",
      "processing of natural language data (text). Though \"stop words\" usually \n",
      "refers to the most common words in a language, there is no single universal \n",
      "list of stop words used by all natural language processing tools, and \n",
      "indeed not all tools even use such a list. Some tools specifically avoid \n",
      "removing these stop words to support phrase search.\n",
      "\n",
      "\n",
      "After removing stop words from the said text:\n",
      "['In', 'computing,', 'stop', 'words', 'words', 'filtered', 'processing', 'natural', 'language', 'data', '(text).', 'Though', '\"stop', 'words\"', 'usually', 'refers', 'common', 'words', 'language,', 'single', 'universal', 'list', 'stop', 'words', 'used', 'natural', 'language', 'processing', 'tools,', 'indeed', 'tools', 'even', 'use', 'list.', 'Some', 'tools', 'specifically', 'avoid', 'removing', 'stop', 'words', 'support', 'phrase', 'search.']\n"
     ]
    }
   ],
   "source": [
    "stoplist = stopwords.words('english')\n",
    "text = '''\n",
    "In computing, stop words are words which are filtered out before or after \n",
    "processing of natural language data (text). Though \"stop words\" usually \n",
    "refers to the most common words in a language, there is no single universal \n",
    "list of stop words used by all natural language processing tools, and \n",
    "indeed not all tools even use such a list. Some tools specifically avoid \n",
    "removing these stop words to support phrase search.\n",
    "'''\n",
    "print(\"\\nOriginal string:\")\n",
    "print(text)\n",
    "clean_word_list = [word for word in text.split() if word not in stoplist]\n",
    "print(\"\\nAfter removing stop words from the said text:\")\n",
    "print(clean_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nick likes play football, fond tennis.\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "text = \"Nick likes to play football, however he is not too fond of tennis.\"\n",
    "filtered_sentence = remove_stopwords(text)\n",
    "\n",
    "print(filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sal ut les loulous!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "strip_multiple_whitespaces(\"sal    ut\" + '\\r' + \" les\" + '\\n' + \"         loulous!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.parsing.preprocessing import strip_tags\n",
    "\n",
    "def remove_tags(inStr):\n",
    "    \"\"\"Removes all the tags and markup e.g. <p> </p>.\"\"\"\n",
    "    return strip_tags(inStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HAHAHA'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_tags(\"<h1>HAHAHA</h1>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sage",
   "language": "python",
   "name": "sage"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
